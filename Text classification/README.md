# Задача проекта

Решить задачу бинарной классификации текстов, необходимо достичь минимального показателя метрики F1 - 0,75 на тестовой выборке

# Используемые модели в проекте:

- Логистические регрессии, обученные на count-vectorizer и tf-idf
- Модели Наивного Байеса, обученные на count-vectorizer и tf-idf
- Логистическая регрессия на обученных эмбедингах с помощью Distil-Bert

# Метрика качества:
- F1

# Выводы по проекту:

В ходе проекта была проведена очистка текста:
- Удалены лишние пробелы 
- Приведены все слова к нижнему регистру
- Удалены знаки пунктуации и числа
- Удалены стоп слова
- Лемматизированы получившиеся слова в предложениях

Дальше были построены несколько различных моделей:
- Логистические регрессии с помощью count_vectorizer и tf-idf
- Модели Наивного Байеса с помощью count_vectorizer и tf-idf
- Обучены эмбединги на модели distilbert (обучалась отдельно на gpu в google colab)

Лучший результат по метрике F1 показала модель логистической регрессии с помощью tf-idf (f1 = 0.77)

## Результаты моделей:

|                      Модели                      |   F1   |  
| :-----------------------------------------------:|:--------:|
| Логистическая регрессия (count-vect)             | 0,76 | 
| Логистическая регрессия (tf-idf)                 | 0.77  | 
| Наивный Байес  (count-vect)                      | 0.73  | 
| Наивный Байес  (tf-idf)                          | 0.75  | 
| Логистическая регрессия (эмбединги distil-bert)  | 0.74  | 
 
